{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a66f3175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133294b2",
   "metadata": {},
   "source": [
    "To construct a graph that is light weight enough while still being able to generate insights, the input data will be split into 3 groups: Categorical values to create subgroups, numerical values to act as idenfitifiers to identify the nearest neighbours for each subgroup and the resale price which is the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359058c8",
   "metadata": {},
   "source": [
    "The first step is clean the numeric values that will be used for the k-nn algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11d278ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('data/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c18e1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_df = data_df.copy()\n",
    "# splitting the month and the year allows to check long term trend as well as seasonality in the year itself.\n",
    "listing_df[['year', 'month_2']] = listing_df['month'].str.split('-').to_list()\n",
    "listing_df['year'] = listing_df['year'].astype(int)\n",
    "listing_df['month_2'] = listing_df['month_2'].astype(int)\n",
    "\n",
    "listing_df['month_index'] = listing_df['year'].astype(int)*12 + listing_df['month_2'].astype(int)\n",
    "listing_df['lease_commence_index'] = listing_df['lease_commence_date']*12\n",
    "\n",
    "# recalculate remaining_lease in years, rounding down to the closest year\n",
    "listing_df['remaining_lease'] = np.floor((99 * 12 - (listing_df['month_index'] - listing_df['lease_commence_index']))/12)\n",
    "\n",
    "listing_df['flat_model'] = listing_df.flat_model.replace(\n",
    "listing_df.groupby('flat_model').agg({'resale_price':'mean', 'flat_model':'count'}).rename(columns={'flat_model':'count'}).reset_index().query(\"count < 500\")['flat_model'].to_list(),\n",
    "'OTHERS'\n",
    ")\n",
    "remaining_lease_df = listing_df[['remaining_lease']]\n",
    "\n",
    "\n",
    "listing_df = listing_df.drop(columns=['month', 'block', 'year', 'month_2', 'lease_commence_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f2736e",
   "metadata": {},
   "source": [
    "Categorical values will also be one-hot encoded to aid in the k-nn algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63e8a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = list(listing_df.select_dtypes(exclude='number').columns)\n",
    "for column in ['flat_type', 'flat_model', 'storey_range']:\n",
    "    listing_df = pd.concat([listing_df, pd.get_dummies(listing_df[[column]])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83f449",
   "metadata": {},
   "source": [
    "The numeric values generated in the previous step are scaled to reduce the impact of high variance categories vs low-variance categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "812e8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in listing_df.select_dtypes(include='number').columns:\n",
    "    if column != 'month_index':\n",
    "        listing_df[column] = StandardScaler().fit_transform(listing_df[[column]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c64df",
   "metadata": {},
   "source": [
    "The graph is initialised with values that could inform the resale price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07a424d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for i, row in listing_df.iterrows():\n",
    "    G.add_node(i,\n",
    "                flat_type=row['flat_type'],\n",
    "                flat_model=row['flat_model'],\n",
    "                town=row['town'],\n",
    "                street_name=row['street_name'],\n",
    "                storey_range=row['storey_range'],\n",
    "                remaining_lease=remaining_lease_df.iloc[i],\n",
    "                resale_price=row['resale_price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855ff521",
   "metadata": {},
   "source": [
    "All the listings are grouped over the categorical columns and edges are added between the k-closest values. The other limitation that's applied is to only look at the 12 months around the node being considered. This limits the number of edges, while ensuring that the most relevant are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67a985c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in cat_cols:\n",
    "    for col_name, group in listing_df.groupby(column):\n",
    "\n",
    "        subrows = group.index\n",
    "\n",
    "        k = min(5,len(group)-1)\n",
    "\n",
    "        nn = NearestNeighbors(n_neighbors=k+1)\n",
    "        nn.fit(group.drop(columns=cat_cols+['resale_price', 'month_index']))\n",
    "        distances, indices = nn.kneighbors(group.drop(columns=cat_cols+['resale_price', 'month_index']))\n",
    "\n",
    "        for row_pos, (d, i) in enumerate(zip(distances, indices)):\n",
    "            u = subrows[row_pos]\n",
    "\n",
    "            for dist, nbr_pos in zip(d, i):\n",
    "                if nbr_pos != row_pos:\n",
    "                    v = subrows[nbr_pos]\n",
    "\n",
    "                    w = float(np.exp(-dist))\n",
    "\n",
    "                    if G.has_edge(u, v):\n",
    "                        if w > G[u][v]['weight']:\n",
    "                            G[u][v]['weight'] = w\n",
    "                    else:\n",
    "                        G.add_edge(u,v,weight=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c53e597",
   "metadata": {},
   "source": [
    "While the average degree is 15.3, the median is 14, implying that the majority of the nodes have at least some edges, and the p90 of 21 shows that there are very few nodes which are too heavily connected. The low numnber of isolated nodes means that every node will contribute, and helps in analysing the data.\n",
    "\n",
    "The assortavity shows that people care more about the town as compared to the street_name, and are more inclined to consider the type of house more, i.e. flat_type, flat_model and storey_range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed2cdd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: 80374 nodes, 616276 edges\n",
      "Degree: mean 15.335207903053226 median 14.0 p90 21.0\n",
      "Global clustering (transitivity): 0.3938\n",
      "Assortativity by town: 0.632\n",
      "Assortativity by flat_type: 0.997\n",
      "Assortativity by street_name: 0.42\n",
      "Assortativity by storey_range: 0.93\n",
      "Assortativity by flat_model: 0.993\n",
      "Assortativity by storey_range: 0.93\n"
     ]
    }
   ],
   "source": [
    "num_nodes, num_edges = G.number_of_nodes(), G.number_of_edges()\n",
    "print(f\"Graph: {num_nodes} nodes, {num_edges} edges\")\n",
    "\n",
    "deg = np.array([d for _, d in G.degree()])\n",
    "print(\"Degree: mean\", deg.mean(), \"median\", np.median(deg), \"p90\", np.percentile(deg, 90))\n",
    "\n",
    "trans = nx.transitivity(G)\n",
    "print(\"Global clustering (transitivity):\", round(trans, 4))\n",
    "\n",
    "for column in cat_cols + ['storey_range']:\n",
    "    print(f\"Assortativity by {column}:\", round(nx.attribute_assortativity_coefficient(G, column), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12213244",
   "metadata": {},
   "source": [
    "TASK II\n",
    "\n",
    "The Louvain community allows grouping of nodes into communities. This is done by trying to maximise modularity, which captures how much more densely nodes in a community are connected to each other, compared to random nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e3057098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dcd68483",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community.best_partition(G, weight='weight', resolution=1.0)  # dict: node -> community_id\n",
    "nx.set_node_attributes(G, partition, 'community')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ffeebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_df = pd.DataFrame({'community': list(partition.values())})\n",
    "data_df = pd.concat([data_df, part_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7347cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_communities = pd.DataFrame(data_df.community.value_counts().head(3)).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8375fff7",
   "metadata": {},
   "source": [
    "Extracting the 3 biggest communities and inspecting the profile of the top 3 combinations of town, flat_type and storey_range shows us similarities, for example, the biggest community is mostly 3 rooms between the 1st to 3rd floor, meanwhile, the second biggest community is primarily composed of executive flat and the 3rd biggest is primarily 3-rooms in Punggol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c2602d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>community</th>\n",
       "      <th>town</th>\n",
       "      <th>flat_type</th>\n",
       "      <th>storey_range</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>48</td>\n",
       "      <td>TAMPINES</td>\n",
       "      <td>4 ROOM</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>48</td>\n",
       "      <td>JURONG WEST</td>\n",
       "      <td>4 ROOM</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>48</td>\n",
       "      <td>PASIR RIS</td>\n",
       "      <td>4 ROOM</td>\n",
       "      <td>01 TO 03</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>37</td>\n",
       "      <td>PASIR RIS</td>\n",
       "      <td>EXECUTIVE</td>\n",
       "      <td>04 TO 06</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>37</td>\n",
       "      <td>PASIR RIS</td>\n",
       "      <td>EXECUTIVE</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>37</td>\n",
       "      <td>WOODLANDS</td>\n",
       "      <td>EXECUTIVE</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>59</td>\n",
       "      <td>PUNGGOL</td>\n",
       "      <td>3 ROOM</td>\n",
       "      <td>10 TO 12</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>59</td>\n",
       "      <td>PUNGGOL</td>\n",
       "      <td>3 ROOM</td>\n",
       "      <td>13 TO 15</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>59</td>\n",
       "      <td>PUNGGOL</td>\n",
       "      <td>3 ROOM</td>\n",
       "      <td>07 TO 09</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     community         town  flat_type storey_range  count\n",
       "145         48     TAMPINES     4 ROOM     01 TO 03    282\n",
       "140         48  JURONG WEST     4 ROOM     01 TO 03    225\n",
       "142         48    PASIR RIS     4 ROOM     01 TO 03    201\n",
       "77          37    PASIR RIS  EXECUTIVE     04 TO 06    136\n",
       "78          37    PASIR RIS  EXECUTIVE     07 TO 09    128\n",
       "119         37    WOODLANDS  EXECUTIVE     07 TO 09    116\n",
       "262         59      PUNGGOL     3 ROOM     10 TO 12     82\n",
       "263         59      PUNGGOL     3 ROOM     13 TO 15     70\n",
       "261         59      PUNGGOL     3 ROOM     07 TO 09     61"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = pd.DataFrame(data_df.merge(top_3_communities, on= 'community').groupby(['community', 'town', 'flat_type', 'storey_range'])['community'].count()).rename(columns={'community':'count'}).reset_index()\n",
    "summary.sort_values('count', ascending=False).groupby('community').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e59bfa",
   "metadata": {},
   "source": [
    "Task III\n",
    "\n",
    "The last part of this exercise is to train a GNN to predict resale price, based on the graph network that's been created in the previous steps.\n",
    "\n",
    "To ensure that this training works on as many devices as possible, while using the available resources, the device checks if cuda cores are available, else defaulting to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "146ccccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de0f928",
   "metadata": {},
   "source": [
    "The first step is to prepare the data to be fed into the GNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d476b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7343/2194668649.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  x_arr = X.replace({True:1, False:0}).to_numpy()\n"
     ]
    }
   ],
   "source": [
    "# Removes all categorical and string type columns to prepare data to be used as input into the GNN\n",
    "# Since all the the numerical values have already been standardised for the KNN earlier, no further changesn need to be made\n",
    "train_df = listing_df.drop(columns=['town', 'flat_type', 'street_name', 'storey_range','flat_model'])\n",
    "X = train_df.drop(columns=['resale_price', 'month_index'])\n",
    "y = data_df[['resale_price']]\n",
    "\n",
    "x_arr = X.replace({True:1, False:0}).to_numpy()\n",
    "y_arr = y.to_numpy()\n",
    "\n",
    "node_ids = np.array(train_df.index)\n",
    "id_to_pos = {nid: i for i, nid in enumerate(node_ids)}\n",
    "\n",
    "# Edge tensor is constructed and assigned to the cpu\n",
    "# A undirected, and unweighted graph is constructed to keep the modelling simple\n",
    "edges = []\n",
    "for u, v, data_w in G.edges(data=True):\n",
    "    if u in id_to_pos and v in id_to_pos:\n",
    "        edges.append((id_to_pos[u], id_to_pos[v]))\n",
    "edges = np.array(edges, dtype=np.int64).T\n",
    "# Make undirected explicitly\n",
    "edge_tensor = np.hstack([edges, edges[::-1, :]])\n",
    "edge_tensor = torch.tensor(edge_tensor, dtype=torch.long, device=device)\n",
    "\n",
    "# Convert X and y to tensors and assign to CPU as well\n",
    "x_tensor = torch.tensor(x_arr, dtype=torch.float32, device=device)\n",
    "y_tensor = torch.tensor(y_arr, dtype=torch.float32, device=device)\n",
    "\n",
    "# Split ids based on time; the train_mask will look at data before the month_index 24241\n",
    "# this evaluates back to jan 2020\n",
    "# This split gives an 80:20 split for train and test\n",
    "train_mask = torch.tensor((listing_df.month_index<24241).values, device=device)\n",
    "test_mask = torch.tensor((listing_df.month_index>=24241).values, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e0725",
   "metadata": {},
   "source": [
    "The next step is to ready the data to be fed into the model. Since the size of the graph network is big, a loader will be used to break the graph into smaller batches. While this reduces the training speed, the memory impact is also reduced, making this training more accessible. The values in the loader can be changed to increase/reduce the batch size depending on the computational resources available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a286df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soham/.local/lib/python3.10/site-packages/torch_geometric/sampler/neighbor_sampler.py:61: UserWarning: Using 'NeighborSampler' without a 'pyg-lib' installation is deprecated and will be removed soon. Please install 'pyg-lib' for accelerated neighborhood sampling\n",
      "  warnings.warn(f\"Using '{self.__class__.__name__}' without a \"\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "# Freezing seed to ensure reproducibiilty\n",
    "torch.manual_seed(22)\n",
    "np.random.seed(22)\n",
    "\n",
    "data = Data(x=x_tensor, edge_index=edge_tensor, y=y_tensor, device=device)\n",
    "data.train_mask, data.test_mask = train_mask, test_mask\n",
    "\n",
    "\n",
    "train_idx = torch.where(data.train_mask)[0]\n",
    "test_idx  = torch.where(data.test_mask)[0]\n",
    "\n",
    "# Define loaders to reduce memory consumption\n",
    "train_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[15,10],\n",
    "    input_nodes=train_idx,\n",
    "    batch_size = 8192,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_loader = NeighborLoader(\n",
    "    data,\n",
    "    num_neighbors=[-1,-1],\n",
    "    input_nodes=test_idx,\n",
    "    batch_size = 4096,\n",
    "    shuffle = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf940fd0",
   "metadata": {},
   "source": [
    "The model itself is also defined. The SAGEConv allows to sample and aggregate the information present in the nodes neighbours, as well as higher degrees of connection, depending on the number of layers defined in the model. This method allows not just a single node's information to be used to predict the resale price, but also any nodes that - as previously defined - are considered to be close matches.\n",
    "\n",
    "For this task, I've chosen a relatively light network with only 64 hidden nodes and 2 layers. If more compute is available, these hyper-parameters can be tuned to get better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "482217dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GNN\n",
    "from torch import nn\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class price_model(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=64, layers=2):\n",
    "        super().__init__()\n",
    "        self.layer_list = []\n",
    "\n",
    "        for i in range(layers):\n",
    "            if i == 0:\n",
    "                self.layer_list.append(SAGEConv(in_dim, hidden))\n",
    "            else:\n",
    "                self.layer_list.append(SAGEConv(hidden, hidden))\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, 32), nn.ReLU(), nn.Linear(32,1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # iterate through the layers for the forward pass\n",
    "        for layer in self.layer_list:\n",
    "            x = layer(x, edge_index)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "        return self.head(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfdab48",
   "metadata": {},
   "source": [
    "The last part would be to actually train a model and track how the loss decreases over time. An MSE objective is used to determine how well the model performs, as it's s continuous and differentiable loss function that handles regressions problems well.\n",
    "\n",
    "While just a train and test dataset are used for this, with the test dataset acting as a validation set to determine an early stop, a combination of train, test and validation can be used if multiple hyper parameters or models are to be used to find the model that best reduces the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f49dcbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model as well as optimiser and loss function\n",
    "\n",
    "model = price_model(in_dim = data.num_node_features)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-1, weight_decay=1e-2)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a40a7807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training step and evaluation function\n",
    "\n",
    "def train(model, train_loader, opt, loss_func):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        pred = model(batch.x, batch.edge_index)\n",
    "        loss = loss_func(pred[:batch.batch_size], data.y[:batch.batch_size])\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss/len(train_loader)\n",
    "\n",
    "def eval(model, loss_func, loader):\n",
    "    model.eval()\n",
    "\n",
    "    total_mse = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch.x, batch.edge_index)[:batch.batch_size]\n",
    "            y = batch.y[:batch.batch_size]\n",
    "\n",
    "            total_mse += loss_func(pred, y).item()\n",
    "\n",
    "    return total_mse/len(loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95254bfc",
   "metadata": {},
   "source": [
    "As can be seen from the result of the training, the model is capable of optimising and predicting the resale price over time. An early stop is also evaluated at every 10th epoch to ensure that the model doesn't overfit on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ac83491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch0 | Test RMSE 466,430.9330\n",
      "Epoch10 | Test RMSE 456,669.0031\n",
      "Epoch20 | Test RMSE 423,921.2220\n",
      "Epoch30 | Test RMSE 371,376.5566\n",
      "Epoch40 | Test RMSE 308,402.0449\n",
      "Epoch50 | Test RMSE 248,515.2676\n",
      "Epoch60 | Test RMSE 205,814.0296\n",
      "Epoch70 | Test RMSE 185,463.6202\n",
      "Epoch80 | Test RMSE 179,347.9104\n",
      "Epoch90 | Test RMSE 177,569.1015\n",
      "Epoch100 | Test RMSE 176,076.8792\n",
      "Epoch110 | Test RMSE 174,291.9357\n",
      "Epoch120 | Test RMSE 172,411.3407\n",
      "Epoch130 | Test RMSE 170,601.2078\n",
      "Epoch140 | Test RMSE 168,936.5711\n",
      "Epoch150 | Test RMSE 167,433.5058\n",
      "Epoch160 | Test RMSE 166,089.8325\n",
      "Epoch170 | Test RMSE 164,898.4979\n",
      "Epoch180 | Test RMSE 163,845.9890\n",
      "Epoch190 | Test RMSE 162,921.4957\n",
      "Epoch200 | Test RMSE 162,106.6576\n",
      "Epoch210 | Test RMSE 161,389.8800\n",
      "Epoch220 | Test RMSE 160,759.4743\n",
      "Epoch230 | Test RMSE 160,200.5447\n",
      "Epoch240 | Test RMSE 159,703.1486\n",
      "Epoch250 | Test RMSE 159,258.8177\n",
      "Epoch260 | Test RMSE 158,858.1302\n",
      "Epoch270 | Test RMSE 158,494.0311\n",
      "Epoch280 | Test RMSE 158,160.2954\n",
      "Epoch290 | Test RMSE 157,852.5181\n",
      "Epoch300 | Test RMSE 157,565.6036\n",
      "Epoch310 | Test RMSE 157,296.7609\n",
      "Epoch320 | Test RMSE 157,042.2667\n",
      "Epoch330 | Test RMSE 156,802.8212\n",
      "Epoch340 | Test RMSE 156,573.9661\n",
      "Epoch350 | Test RMSE 156,357.8304\n",
      "Epoch360 | Test RMSE 156,149.3636\n",
      "Epoch370 | Test RMSE 155,950.7713\n",
      "Epoch380 | Test RMSE 155,760.8440\n",
      "Epoch390 | Test RMSE 155,579.1986\n",
      "Epoch400 | Test RMSE 155,404.5777\n",
      "Epoch410 | Test RMSE 155,237.7587\n",
      "Epoch420 | Test RMSE 155,077.7927\n",
      "Epoch430 | Test RMSE 154,924.3556\n",
      "Epoch440 | Test RMSE 154,778.8050\n",
      "Epoch450 | Test RMSE 154,639.3055\n",
      "Epoch460 | Test RMSE 154,506.0345\n",
      "Epoch470 | Test RMSE 154,379.7939\n",
      "Epoch480 | Test RMSE 154,259.6436\n",
      "Epoch490 | Test RMSE 154,145.5495\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation flow\n",
    "best_mse = float('inf')\n",
    "\n",
    "for epoch in range(500):\n",
    "    train(model, train_loader, opt, loss_func)\n",
    "\n",
    "    if epoch%10 == 0:\n",
    "        # train_mse = eval(model, loss_func, train_loader)\n",
    "        test_mse = eval(model, loss_func, test_loader)\n",
    "    \n",
    "        print(f\"Epoch{epoch} | Test RMSE {np.sqrt(test_mse):,.4f}\")\n",
    "\n",
    "        if test_mse < best_mse:\n",
    "            best_mse = test_mse\n",
    "        \n",
    "        if test_mse > best_mse:\n",
    "            break\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
